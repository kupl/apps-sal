["def main():\n\tt = int(input())\n\treserved = set(input().split())\n\n\tlines = int(input())\n\tcode = ''\n\tfor i in range(lines):\n\t\tcode += input() + '\\n'\n\n\tdef is_word(suspect):\n\t\tif suspect[0].isdigit(): return False\n\t\tfor x in suspect:\n\t\t\tif (not x.isalpha()) and (not x in {'_', '$'}) and (not x.isdigit()):\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef is_token(suspect):\n\t\tif suspect in reserved: return True\n\t\tif is_word(suspect): return True\n\t\tif suspect.isdigit(): return True\n\t\treturn False\n\n\tdef remove_comments(code):\n\t\trez = ''\n\t\tstate = None\n\t\tfor i in range(len(code)):\n\t\t\tif code[i] == '#':\n\t\t\t\tstate = 'comment'\n\t\t\telif code[i] == '\\n':\n\t\t\t\trez += code[i]\n\t\t\t\tstate = None\n\t\t\telse:\n\t\t\t\tif state != 'comment':\n\t\t\t\t\trez += code[i]\n\t\treturn rez\n\n\tcode = remove_comments(code)\n\tcode = code.replace('\\n', ' ')\n\n\tdef split(code):\n\t\ttokens = []\n\t\ti = 0\n\t\twhile i < len(code):\n\t\t\tif code[i] == ' ':\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\t\t\tfor l in range(min(len(code), 90), 0, -1):\n\t\t\t\tif is_token(code[i:i + l]):\n\t\t\t\t\ttokens.append(code[i:i + l])\n\t\t\t\t\ti += l\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn []\n\t\treturn tokens\n\n\tdef minmize(tokens):\n\t\tall = []\n\t\tpref = [chr(i + ord('a')) for i in range(26)]\n\t\tall.extend(pref)\n\t\tfor ext in range(3):\n\t\t\tcur = []\n\t\t\tfor i in range(26):\n\t\t\t\tfor p in pref:\n\t\t\t\t\tcur.append(p + chr(i + ord('a')))\n\t\t\tcur.sort()\n\t\t\tall.extend(cur)\n\t\t\tpref = cur[::]\n\n\t\tall.reverse()\n\t\tzip = dict()\n\t\tfor i in range(len(tokens)):\n\t\t\tif not tokens[i] in reserved and not tokens[i][0].isdigit():\n\t\t\t\tif not zip.get(tokens[i], None):\n\t\t\t\t\twhile all[-1] in reserved:\n\t\t\t\t\t\tall.pop()\n\t\t\t\t\tzip[tokens[i]] = all[-1]\n\t\t\t\t\tall.pop()\n\t\t\t\ttokens[i] = zip[tokens[i]]\n\t\treturn tokens\n\n\ttokens = (minmize(split(code)))\n\n\tdef cmp(a, b):\n\t\tif len(a) != len(b): return False\n\t\tfor i in range(len(a)):\n\t\t\tif a[i] != b[i]: return False\n\t\treturn True\n\n\tfinal = []\n\tfor i in range(len(tokens)):\n\t\tp = 0\n\t\tfor j in range(i - 1, -1, -1):\n\t\t\tif len(''.join(tokens[j:i + 1])) > 23:\n\t\t\t\tp = j\n\t\t\t\tbreak\n\n\t\tst = ''\n\t\tif i and (not cmp(tokens[p:i + 1], split(''.join(final[p:i]) + tokens[i]))):\n\t\t\tst += ' '\n\t\tfinal.append(st + tokens[i])\n\tprint(''.join(final))\nmain()", "from collections import defaultdict\n\nn = int(input())\nreserved = set(input().split())\n\nm = int(input())\nlines = [ input().split('#')[0].split() for _ in range(m) ]\n\ntokens = sum(lines, [])\n\ndef isword(t):\n\treturn not t[0].isdigit() and all(c.isalnum() or c == '_' or c == '$' for c in t)\n\ndef ok(t):\n\treturn t in reserved or t.isdigit() or isword(t)\n\t\ndef tokenize(t):\n\tres = []\n\ti = 0\n\tn = len(t)\n\twhile i < n:\n\t\tfor j in range(n, i, -1):\n\t\t\tif ok(t[i:j]):\n\t\t\t\tres.append(t[i:j])\n\t\t\t\ti = j\n\t\t\t\tbreak\n\treturn res\n\t\ntokens = sum(map(tokenize, tokens), [])\n\ndig = 1\nk = 0\nlimit = 26\n\ndef next_token():\n\tnonlocal dig, k, limit\n\tif k == limit:\n\t\tk = 0\n\t\tdig += 1\n\t\tlimit = 26 ** dig\n\tw = []\n\tl = k\n\tfor _ in range(dig):\n\t\tw += chr(ord('a') + l % 26)\n\t\tl //= 26\n\tw = ''.join(reversed(w))\n\tk += 1\n\tif w in reserved:\n\t\treturn next_token()\n\treturn w\n\t\t\ndc = defaultdict(next_token)\n\ndef replace(t):\n\tif t in reserved or t.isdigit():\n\t\treturn t\n\treturn dc[t]\n\ndef can_append(a, b, bg):\n\tif len(a) == 0:\n\t\treturn True\n\t#if a not in reserved and b not in reserved:\n\t#\treturn False\n\tfor i, s in enumerate(bg):\n\t\tif i + 1 != len(bg) and len(a) - s > 21:\n\t\t\tcontinue\n\t\tfor i in range(len(b)):\n\t\t\tif ok(a[s:] + b[:i+1]):\n\t\t\t\t#print(a, b, a[s:] + b[:i+1])\n\t\t\t\treturn False\n\t#print('can append', a, b)\n\treturn True\t\n\nres = ['']\nbegin = []\nfor t in map(replace, tokens):\n\tif not can_append(res[-1], t, begin):\n\t\tres.append('')\n\t\tbegin = []\n\tbegin.append(len(res[-1]))\n\tres[-1] += t\n\nprint(' '.join(res))", "#!/usr/bin/env python3\nimport sys\n\nn, reserved, m, *lines = sys.stdin.readlines()\nn = int(n)\nreserved = set(reserved.split())\nm = int(m)\n#assert len(lines) == m\n\ndef strip_comment(line):\n    return line[:-1].split('#', maxsplit=1)[0]\nlines = ' '.join(map(strip_comment, lines))\n\n#print(repr(lines))\n\ndef is_digit(c):\n    return '0' <= c <= '9'\ndef is_word_char(c):\n    return c == '_' or c == '$' or '0' <= c <= '9' or 'A' <= c <= 'Z' or 'a' <= c <= 'z'\n\ndef digit_match(s, ind):\n    assert ind < len(s)\n    res = 0\n    while res + ind < len(s) and is_digit(s[res + ind]):\n        res += 1\n    return res\n\ndef word_match(s, ind):\n    assert ind < len(s)\n    if is_digit(s[ind]):\n        return 0\n    res = 0\n    while res + ind < len(s) and is_word_char(s[res + ind]):\n        res += 1\n    return res\n\ndef reserved_match(s, ind):\n    assert ind < len(s)\n    return max((len(r) for r in reserved if s.startswith(r, ind)), default=0)\n\ndef tokenize(s):\n    ind = 0\n    while ind < len(s):\n        if s[ind] == ' ':\n            ind += 1\n            continue\n        l = max(digit_match(s, ind), word_match(s, ind), reserved_match(s, ind))\n        if l == 0:\n            yield '\\0' # yield a garbage character to mess up the stream\n            return\n        yield s[ind:ind+l]\n        ind += l\n\ndef simplify_tokens(tokens):\n    def lex_next(s):\n        for i in range(len(s)-1, -1, -1):\n            assert 'a' <= s[i] <= 'z'\n            if s[i] < 'z':\n                return s[:i] + chr(ord(s[i])+1) + 'a' * (len(s) - i - 1)\n        return 'a' * (len(s)+1)\n    converted = {}\n    cur_word = ''\n    for token in tokens:\n        if token in reserved:\n            yield token\n        elif '0' <= token[0] <= '9':\n            yield token\n        elif token in converted:\n            yield converted[token]\n        else:\n            cur_word = lex_next(cur_word)\n            while cur_word in reserved:\n                cur_word = lex_next(cur_word)\n\n            converted[token] = cur_word\n            yield cur_word\n\ntokens = list(simplify_tokens(tokenize(lines)))\n#print(tokens)\n\ncur_tokens = []\nresult = []\nfor token in tokens:\n    #assert token\n    cur_tokens.append(token)\n    # only have to check the last 20 tokens\n    if list(tokenize(''.join(cur_tokens[-21:]))) != cur_tokens[-21:]:\n        result.append(''.join(cur_tokens[:-1]))\n        cur_tokens = [token]\n#assert cur_tokens\nif cur_tokens:\n    result.append(''.join(cur_tokens))\nprint(' '.join(result))\n", "#!/usr/bin/env python3\nimport sys\n\nn, reserved, m, *lines = sys.stdin.readlines()\nn = int(n)\nreserved = set(reserved.split())\nm = int(m)\n#assert len(lines) == m\n\ndef strip_comment(line):\n    return line[:-1].split('#', maxsplit=1)[0]\nlines = ' '.join(map(strip_comment, lines))\n\n#print(repr(lines))\n\ndef is_digit(c):\n    return '0' <= c <= '9'\ndef is_word_char(c):\n    return c == '_' or c == '$' or '0' <= c <= '9' or 'A' <= c <= 'Z' or 'a' <= c <= 'z'\n\ndef digit_match(s, ind):\n    assert ind < len(s)\n    res = 0\n    while res + ind < len(s) and is_digit(s[res + ind]):\n        res += 1\n    return res\n\ndef word_match(s, ind):\n    assert ind < len(s)\n    if is_digit(s[ind]):\n        return 0\n    res = 0\n    while res + ind < len(s) and is_word_char(s[res + ind]):\n        res += 1\n    return res\n\ndef reserved_match(s, ind):\n    assert ind < len(s)\n    return max((len(r) for r in reserved if s.startswith(r, ind)), default=0)\n\ndef tokenize(s):\n    ind = 0\n    while ind < len(s):\n        if s[ind] == ' ':\n            ind += 1\n            continue\n        l = max(digit_match(s, ind), word_match(s, ind), reserved_match(s, ind))\n        if l == 0:\n            yield '\\0' # yield a garbage character to mess up the stream\n            return\n        yield s[ind:ind+l]\n        ind += l\n\ndef simplify_tokens(tokens):\n    def lex_next(s):\n        for i in range(len(s)-1, -1, -1):\n            assert 'a' <= s[i] <= 'z'\n            if s[i] < 'z':\n                return s[:i] + chr(ord(s[i])+1) + 'a' * (len(s) - i - 1)\n        return 'a' * (len(s)+1)\n    converted = {}\n    cur_word = ''\n    for token in tokens:\n        if token in reserved:\n            yield token\n        elif '0' <= token[0] <= '9':\n            yield token\n        elif token in converted:\n            yield converted[token]\n        else:\n            cur_word = lex_next(cur_word)\n            while cur_word in reserved:\n                cur_word = lex_next(cur_word)\n\n            converted[token] = cur_word\n            yield cur_word\n\ntokens = list(simplify_tokens(tokenize(lines)))\n#print(tokens)\n\ncur_tokens = []\nresult = []\nfor token in tokens:\n    #assert token\n    cur_tokens.append(token)\n    # only have to check the last 20 tokens\n    if list(tokenize(''.join(cur_tokens[-21:]))) != cur_tokens[-21:]:\n        result.append(''.join(cur_tokens[:-1]))\n        cur_tokens = [token]\n#assert cur_tokens\nif cur_tokens:\n    result.append(''.join(cur_tokens))\nprint(' '.join(result))\n"]