["from functools import reduce\n\nclass Datamining:\n\n    def __init__(self, train_set):\n        self.p = train_set[:5]\n    \n    def lagrange_interp(self, x):\n        return sum(reduce(lambda p,n: p*n, [(x-xi)/(xj-xi) for (i,(xi,yi)) in enumerate(self.p) if j!=i], yj) for (j,(xj,yj)) in enumerate(self.p))\n    \n    def predict(self, x):\n        return self.lagrange_interp(x)", "# Matrix access functions\nrow = lambda m, r: m[r]\ncol = lambda m, c: [r[c] for r in m]\n\n# Matrix row functions\nrow_mul = lambda m, r, coeff: [[v * coeff for v in row] if rr == r else row for rr, row in enumerate(m)]\nrow_add = lambda m, r1, r2, coeff=1: [[v1 + v2 * coeff for v1, v2 in zip(row, m[r2])] if rr == r1 else row for rr, row in enumerate(m)]\n\nclass Datamining:\n\n    def __init__(self, train_set):\n        \"\"\"\n        Given the training set build a set of polynomial regression coefficients\n        Up to a maximum of degree 10.\n        Yay for linear algebra!\n        \"\"\"\n        N = len(train_set)\n        P = N - 1\n        P = min(10, P)\n        xs, ys = [t[0] for t in train_set], [t[1] for t in train_set]\n        \n        # Build powers of xs[i]\n        m = [[sum(xs[i] ** (p1 + p2) for i in range(N)) for p2 in range(P)] for p1 in range(P)]\n        # Create augmented matrix with expected coefficients column\n        r = [sum(ys[i] * xs[i] ** p for i in range(N)) for p in range(P)]\n        m = [m[i] + [r[i]] for i in range(P)]\n        \n        # Perform Gaussian reduction\n        for r in range(P):\n            # Make current diagonal equal 1\n            if m[r][r] == 0:\n                # Oops this diagonal is zeo, so add another row\n                vs = col(m, r)\n                for rr, v in enumerate(vs):\n                    if rr > r and v != 0:\n                        break\n                m = row_add(m, r, rr, 1/m[rr][r])\n            else:\n                # Multiply to get to value 1\n                m = row_mul(m, r, 1/m[r][r])\n            \n            # Use current row to zero out other rows on same column\n            for cr in range(P):\n                if cr != r:\n                    f = m[cr][r]\n                    if f != 0:\n                        m = row_add(m, cr, r, -f)\n\n        self.coeffs = col(m, -1)\n        print((\"coeffs: {}\".format([round(c, 4) for c in self.coeffs])))\n        \n    def predict(self, x):\n        \"\"\"Use calculated coefficients to predict next y given x\"\"\"\n        return sum(c * (x ** p) for p, c in enumerate(self.coeffs))\n", "def reduced_echelon_form(matrix):\n    matrix = matrix[::]\n    if not matrix: return\n    lead = 0\n    row_count = len(matrix)\n    column_count = len(matrix[0])\n    for r in range(row_count):\n        if lead >= column_count:\n            return matrix\n        i = r\n        while matrix[i][lead] == 0:\n            i += 1\n            if i == row_count:\n                i = r\n                lead += 1\n                if column_count == lead:\n                    return matrix\n        matrix[i], matrix[r] = matrix[r], matrix[i]\n        lv = matrix[r][lead]\n        matrix[r] = [mrx / float(lv) for mrx in matrix[r]]\n        for i in range(row_count):\n            if i != r:\n                lv = matrix[i][lead]\n                matrix[i] = [iv - lv * rv for rv, iv in zip(matrix[r], matrix[i])]\n        lead += 1\n\n    return matrix\n\nclass Poly:\n    def __init__(self, k, coef, lambda_=[1]):\n        vector = [(_[1], e)for e, _ in enumerate( zip(range(0, k+1), coef) )]  # _[1]:coef\n        self.poly = vector\n        self.lambda_ = lambda_\n\n    def eval_poly(self, x):\n        return sum(l* pe[0]*x**pe[1] for pe, l in zip(self.poly, self.lambda_))\n\n    def _to_string(self):\n        s = \"\"\n        for p, e in self.poly[::-1]:\n            s += \"{}{}x^{}\" . format(\"+\" if p>=0 else \"\", p, e)\n        s = s.strip(\"+\")\n        return s\n\nclass Datamining:\n\n    def __init__(self, train_set):\n        self.train = train_set\n        self.copy = train_set[::]\n        self._learn()\n\n    def _mse(self, corr, pred):\n        s = [(c - p) ** 2 for c, p in zip(corr, pred)]\n        return sum(s) / len(s)\n\n    def frange(self, start, end, step):\n        while start < end:\n            yield start\n            start += step\n\n    def _learn(self):\n        from random import shuffle\n        train_perc = 0.65\n        iterations = 18\n        best = set()\n        deg = 0\n        lmbds = self.frange(0.7, 1.3, 0.1)\n        degrees = range(2,6)\n        for l in lmbds:\n            for d in degrees:\n                for iter in range(iterations):\n                    # make matrix\n                    # required deg+1 points\n                    m = []\n                    tmp = []\n                    for p in self.train[:deg+1+d]:\n                        x = p[0]\n                        y = p[1]\n                        tmp = [x**e for e in range(deg+1+d)]\n                        tmp.append(y)\n                        m.append(tmp)\n\n                    # reduced form\n                    mtx = reduced_echelon_form(m)\n                    coef = [r[-1] for r in mtx]\n\n                    shuffle(self.copy)\n\n                    ntrain = int(len(self.copy)*train_perc)\n                    train = self.copy[:ntrain]\n                    test = self.copy[ntrain:]\n\n                    poly = Poly(deg+d, coef, lambda_=[l for _ in range(deg+d)])\n                    model = poly.eval_poly\n\n                    predicted = [model(p[0]) for p in test]\n                    correct = [p[1] for p in test]\n\n                    best.add( (self._mse(predicted, correct), poly) )\n\n        _, poly = min(best, key=lambda x: x[0])\n        self.model = poly.eval_poly\n\n    def predict(self, x):\n        return self.model(x)", "from functools import reduce\n\nclass Datamining:\n    def __init__(self, train_set):\n        self.s = train_set[:6]\n\n    def predict(self, x):\n        f = lambda j, x, xj: [(x - xi) / (xj - xi) for (i, (xi,yi)) in enumerate(self.s) if j != i]\n        return sum(\n            reduce(lambda s, n: s * n, f (j,x,xj), yj) for (j, (xj,yj)) in enumerate(self.s))", "from functools import reduce\n\nclass Datamining:\n    def __init__(self, train_set):\n        self.s = train_set[:6]\n\n    def predict(self, x):\n        # Lagrange interpolating polynomial\n        return sum(\n            reduce(lambda s, n: s * n, [(x - xi) / (xj - xi) for (i, (xi,yi)) in enumerate(self.s) if j != i], yj) for\n            (j, (xj,yj)) in enumerate(self.s))", "def matrix_multiply(A,B):\n    rowsA = len(A)\n    colsA = len(A[0])\n    rowsB = len(B)\n    colsB = len(B[0])\n    if colsA != rowsB:\n        print('Number of A columns must equal number of B rows.')\n        return\n    C = [[0],\n         [0],\n         [0],\n         [0],\n         [0]]\n    for i in range(rowsA):\n        for j in range(colsB):\n            total = 0\n            for ii in range(colsA):\n                total += A[i][ii] * B[ii][j]\n            C[i][j] = total\n    return C\n\ndef invert_matrix(A, tol=None):\n    n = len(A)\n    AM = A\n    I = [[1,0,0,0,0],\n          [0,1,0,0,0],\n          [0,0,1,0,0],\n          [0,0,0,1,0],\n          [0,0,0,0,1]]\n    IM = [[1,0,0,0,0],\n          [0,1,0,0,0],\n          [0,0,1,0,0],\n          [0,0,0,1,0],\n          [0,0,0,0,1]]\n    indices = list(range(n))\n    for fd in range(n):\n        fdScaler = 1.0 / AM[fd][fd]\n        for j in range(n):\n            AM[fd][j] *= fdScaler\n            IM[fd][j] *= fdScaler\n        for i in indices[0:fd] + indices[fd+1:]: \n            crScaler = AM[i][fd]\n            for j in range(n): \n                AM[i][j] = AM[i][j] - crScaler * AM[fd][j]\n                IM[i][j] = IM[i][j] - crScaler * IM[fd][j]\n    return IM\n\nmean = lambda x: sum(x)/len(x)\n\nclass Datamining:\n\n    def __init__(self, train_set):\n        A = []\n        v = []\n        for k in range(5):\n            v.append( [mean([y * x**k for x,y in train_set])] )\n            A.append( [mean([x**(j+k) for x,y in train_set]) for j in range(5)] )\n        B = invert_matrix(A)\n        self.coefs = matrix_multiply(B,v)\n\n    def predict(self, x):\n        return sum([ a[0] * x**i for i,a in enumerate(self.coefs)])\n", "class Datamining:\n\n    def __init__(self, train_set):\n        self.X = [s[0] for s in train_set[:6]]\n        self.Y = [s[1] for s in train_set[:6]]\n        \n\n    def predict(self, x):\n        y = 0\n        for j in range(len(self.X)):\n            Lj, xj = 1, self.X[j]\n            for k in range(len(self.X)):\n                xk = self.X[k]\n                if xk != xj:\n                    Lj *= (x-xk)/(xj-xk)\n            y += Lj * self.Y[j]\n        \n        \n        \n        \n        return y\n", "from statistics import mean, pvariance\nfrom copy import deepcopy\n\ndef matrix_transpose(m):\n    return [list(i) for i in zip(*m)]\n    \ndef matrix_matrix_multiply(a, b):\n    n, p, m = len(a), len(b), len(b[0])\n    c = [[0] * m for i in range(n)]\n    \n    for i in range(n):\n        for j in range(m):\n            for k in range(p):\n                c[i][j] += a[i][k] * b[k][j]\n                \n    return c\n\ndef matrix_vector_multiply(a, b):\n    n, p = len(a), len(b)\n    c = [0] * n\n    \n    for i in range(n):\n        for k in range(p):\n            c[i] += a[i][k] * b[k]\n            \n    return c\n    \n# Very crude Gaussian elimination, little regard to performance or accuracy.\ndef matrix_solve(x, y):\n    n, u, v = len(x), deepcopy(x), deepcopy(y)\n\n    for i in range(n - 1):\n        for j in range(i + 1, n):\n            k = u[j][i] / u[i][i]\n            u[j] = [a - k * b for a, b in zip(u[j], u[i])]\n            v[j] = v[j] - k * v[i]\n\n    for i in reversed(range(n)):\n        v[i] = (v[i] - sum([v[j] * u[i][j] for j in range(i + 1, n)])) / u[i][i]\n\n    return v\n    \ndef features(x):\n    return [1, x, x ** 2, x ** 3, x ** 4, x ** 5]\n    \nclass Datamining:\n    def __init__(self, train_set):\n        x, y = zip(*train_set)\n        \n        design = [features(i) for i in x]\n        t_design = matrix_transpose(design)\n        \n        cov = matrix_matrix_multiply(t_design, design)\n         \n        self.beta_hat = matrix_solve(cov, matrix_vector_multiply(t_design, y))\n\n    def predict(self, x):\n        z = features(x)\n        \n        return sum([a * b for a, b in zip(z, self.beta_hat)])", "\n# https://integratedmlai.com/matrixinverse/\ndef identity_matrix(n):\n    \"\"\"\n    Creates and returns an identity matrix.\n        :param n: the square size of the matrix\n        :returns: a square identity matrix\n    \"\"\"\n    I = zeros_matrix(n, n)\n    for i in range(n):\n        I[i][i] = 1.0\n\n    return I\n\ndef invert_matrix(A, tol=None):\n    \"\"\"\n    Returns the inverse of the passed in matrix.\n        :param A: The matrix to be inversed\n\n        :return: The inverse of the matrix A\n    \"\"\"\n    n = len(A)\n    AM = copy_matrix(A)\n    I = identity_matrix(n)\n    IM = copy_matrix(I)\n\n    # Section 3: Perform row operations\n    indices = list(range(n)) # to allow flexible row referencing ***\n    for fd in range(n): # fd stands for focus diagonal\n        fdScaler = 1.0 / AM[fd][fd]\n        # FIRST: scale fd row with fd inverse.\n        for j in range(n): # Use j to indicate column looping.\n            AM[fd][j] *= fdScaler\n            IM[fd][j] *= fdScaler\n        # SECOND: operate on all rows except fd row as follows:\n        for i in indices[0:fd] + indices[fd+1:]:\n            # *** skip row with fd in it.\n            crScaler = AM[i][fd] # cr stands for \"current row\".\n            for j in range(n):\n                # cr - crScaler * fdRow, but one element at a time.\n                AM[i][j] = AM[i][j] - crScaler * AM[fd][j]\n                IM[i][j] = IM[i][j] - crScaler * IM[fd][j]\n    return IM\n\n\ndef zeros_matrix(rows, cols):\n    A = []\n    for i in range(rows):\n        A.append([])\n        for j in range(cols):\n            A[-1].append(0.0)\n    return A\n\ndef copy_matrix(M):\n    rows = len(M)\n    cols = len(M[0])\n\n    MC = zeros_matrix(rows, cols)\n\n    for i in range(rows):\n        for j in range(rows):\n            MC[i][j] = M[i][j]\n\n    return MC\n\ndef matrix_multiply(A,B):\n    rowsA = len(A)\n    colsA = len(A[0])\n\n    rowsB = len(B)\n    colsB = len(B[0])\n\n    if colsA != rowsB:\n        print('Number of A columns must equal number of B rows.')\n        return\n\n    C = zeros_matrix(rowsA, colsB)\n\n    for i in range(rowsA):\n        for j in range(colsB):\n            total = 0\n            for ii in range(colsA):\n                total += A[i][ii] * B[ii][j]\n            C[i][j] = total\n\n    return C\n\ndef transposeMatrix(m):\n    return list(map(list,list(zip(*m))))\n\nclass Datamining:\n    def __init__(self, train_set, n=6):\n        self.data = train_set\n        self.n = n\n        self.m = len(self.data)\n        self.fit()\n\n    def fit(self):\n        X, ys = [], []\n        for x, y in self.data:\n            X.append([x ** i for i in range(self.n)])\n            ys.append([y])\n        Xt = transposeMatrix(X)\n        iv = invert_matrix(matrix_multiply(Xt, X))\n        R = matrix_multiply(iv, Xt)\n        self.weights = matrix_multiply(R, ys)\n\n    def predict(self, x):\n        xs = [[x ** i for i in range(self.n)]]\n        return matrix_multiply(xs, self.weights)[0][0]\n", "def minv(mat):\n    N=len(mat)\n    for i in range(N):\n        extra=[0]*N\n        extra[i]=1\n        mat[i].extend(extra)\n    for i in range(N):\n        for j in range(N):\n            if i==j:continue\n            mul=mat[j][i]/mat[i][i]\n            for k in range(N*2):\n                mat[j][k]-=mul*mat[i][k]\n    for i in range(N):\n        div=mat[i][i]\n        for j in range(2*N):\n            mat[i][j]/=div\n    for i in range(N):\n        del mat[i][:N]\n    return mat\n\n\nclass Datamining:\n    def __init__(self, train_set):\n        self.N=len(train_set)\n        if self.N<6: \n            return\n        self.Xt=[[],[],[],[],[],[]]\n        self.y=[]\n        \n        for i in train_set:\n            for k in range(6):\n                self.Xt[k].append(i[0]**k)\n            self.y.append(i[1])\n        \n        self.XtX = [[0]*6 for i in range(6)]\n        for i in range(6):\n            for j in range(6):\n                self.XtX[i][j]=sum(self.Xt[i][k]*self.Xt[j][k] for k in range(self.N))\n        self.XtY = [0]*6\n        for i in range(6):\n            self.XtY[i] = sum(self.Xt[i][k]*self.y[k] for k in range(self.N))\n        \n        self.XtX1=minv(self.XtX)\n        self.b=[0]*6\n        for i in range(6):\n            self.b[i] = sum(self.XtX1[i][k]*self.XtY[k] for k in range(6))\n        \n\n    def predict(self, x):\n        if self.N==5: return 5\n        return sum(self.b[i]*x**i for i in range(6))\n"]